{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1eb412f7-70b6-4823-8bd3-1c2df0966374",
   "metadata": {},
   "source": [
    "# Regex exos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca108ace-b96e-405c-8125-d36863d230f1",
   "metadata": {},
   "source": [
    "## Exo 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27822b66-95cc-4289-a07e-10ac6b64b4f2",
   "metadata": {},
   "source": [
    "### Problem: Write a regex to match the word \"hello\" in a given string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f790a64-5510-468c-9aa7-0a4dfe3efd4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"hello world\"\n",
    "pattern = r'hello'\n",
    "match = re.search(pattern, text)\n",
    "print(match.group())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8171df1-6829-47e1-940c-69145c7ca11b",
   "metadata": {},
   "source": [
    "##  Exo 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30793d5-2d35-46fa-abe3-69f6e855f2c2",
   "metadata": {},
   "source": [
    "### Problem: Write a regex to find all digits in a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "739bf31a-efc7-406e-98a0-8e221326135c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '2', '3', '4', '5']\n"
     ]
    }
   ],
   "source": [
    "text = \"My phone number is 12345.\"\n",
    "pattern = r'\\d'\n",
    "matches = re.findall(pattern, text)\n",
    "print(matches)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0a141c-cef9-4e5c-9eb4-1de950a7c3b2",
   "metadata": {},
   "source": [
    "## Exo 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5254263a-9dfc-4138-915c-38ac102fd258",
   "metadata": {},
   "source": [
    "### Problem : Write a regex to extract all words that end with \"ing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4eaabe29-febd-4ad9-9126-dfcdf9eae0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['running', 'jumping', 'singing']\n"
     ]
    }
   ],
   "source": [
    "text = \"I am running and jumping while singing.\"\n",
    "pattern = r'\\b\\w+ing\\b'\n",
    "matches = re.findall(pattern, text)\n",
    "print(matches)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bd8d14-dab3-4064-aafe-10ed425e438e",
   "metadata": {},
   "source": [
    "# Tokenization Exos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff79cb44-46ee-4f81-a55b-51c957957a0b",
   "metadata": {},
   "source": [
    "## Exo 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0403b3a0-1059-413e-ae32-680a82387b4b",
   "metadata": {},
   "source": [
    "### Problem: Write a Python script that tokenizes a simple sentence into individual words using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5025ca7-665b-4c67-bf04-a6f87baca36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/mohamed/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "#use word_tokenize on any paragraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17047f7-7fd5-4f44-a79b-c4642dc4bb7e",
   "metadata": {},
   "source": [
    "## Exo 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236d87b0-3b00-4768-8396-b653597ce37a",
   "metadata": {},
   "source": [
    "### Problem: Write a Python script that tokenizes a paragraph into individual sentences using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19cd2185-5ac2-4e51-b4b4-2ab14ceb288a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/mohamed/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "#use sent_tokenize on any paragraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bad3e75-dac1-46a7-8e8b-3dfdb4c3e336",
   "metadata": {},
   "source": [
    "## Exo 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05bd627-7927-4ff7-b777-1b8cf76e6edf",
   "metadata": {},
   "source": [
    "### Problem: Write a Python script that tokenizes a sentence into individual words using NLTK. Then, use a regular expression with grouping to extract pairs of the format \"Year-Month\" (e.g., \"2023-08\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b06aab06-526e-47cd-bb4b-ddb187a2964e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year-Month: [('2022', '05'), ('2023', '08')]\n"
     ]
    }
   ],
   "source": [
    "text = \"The project started in 2022-05-10 and was completed by 2023-08.\"\n",
    "pattern = r'(\\d{4})-(\\d{2})'\n",
    "matches = re.findall(pattern, text)\n",
    "print(\"Year-Month:\", matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c633402f-a243-46e1-9a29-457912dc3ce9",
   "metadata": {},
   "source": [
    "# Stemming and Lemmatization exos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820abc5b-e78f-4c35-9d08-a4fe675dc053",
   "metadata": {},
   "source": [
    "## Exo 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28570502-5457-4dc9-b8fb-7146a111ec0d",
   "metadata": {},
   "source": [
    "### Problem: Write a Python script that uses the PorterStemmer to stem a list of words. Display the original words and their stemmed forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12075573-93fc-4030-8eba-ccdf28098edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: running, Stemmed: run\n",
      "Original: jumps, Stemmed: jump\n",
      "Original: easily, Stemmed: easili\n",
      "Original: faster, Stemmed: faster\n",
      "Original: runner, Stemmed: runner\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "# List of words to stem\n",
    "words = [\"running\", \"jumps\", \"easily\", \"faster\", \"runner\"]\n",
    "# Stem each word and print the results\n",
    "for word in words:\n",
    "    stemmed_word = stemmer.stem(word)\n",
    "    print(f\"Original: {word}, Stemmed: {stemmed_word}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c037ca-fe19-4c9d-8922-d505cc588653",
   "metadata": {},
   "source": [
    "## Exo 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb2a93e-8b87-46a4-8b6f-3c971a93b106",
   "metadata": {},
   "source": [
    "### Problem: Write a Python script that uses the WordNetLemmatizer to lemmatize a list of words, considering both nouns and verbs. Display the original words and their lemmatized forms. note that for pos n stands for noun and v for Verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a957a11b-3b9d-49b7-a838-ec3aeff8fd67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/mohamed/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/mohamed/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: running, Lemmatized (Verb): run, Lemmatized (Noun): running\n",
      "Original: jumps, Lemmatized (Verb): jump, Lemmatized (Noun): jump\n",
      "Original: leaves, Lemmatized (Verb): leave, Lemmatized (Noun): leaf\n",
      "Original: better, Lemmatized (Verb): better, Lemmatized (Noun): better\n",
      "Original: easily, Lemmatized (Verb): easily, Lemmatized (Noun): easily\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# Download WordNet data if not already downloaded\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "# Initialize the WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# List of words to lemmatize\n",
    "words = [\"running\", \"jumps\", \"leaves\", \"better\",\"easily\"]\n",
    "# Lemmatize each word as a verb and noun and print the results\n",
    "for word in words:\n",
    "    lemmatized_verb = lemmatizer.lemmatize(word, pos='v')  # Verb\n",
    "    lemmatized_noun = lemmatizer.lemmatize(word, pos='n')  # Noun\n",
    "    print(f\"Original: {word}, Lemmatized (Verb): {lemmatized_verb}, Lemmatized (Noun): {lemmatized_noun}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36567406-6b0a-4040-b27c-76ffc4d7d2a5",
   "metadata": {},
   "source": [
    "## Exo 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0fbc56-9144-4018-84d8-f856a3c8a1ea",
   "metadata": {},
   "source": [
    "### Problem: Write a Python script that compares the results of stemming and lemmatization for a list of words. Analyze cases where the results differ significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a805533b-1e7e-48f4-b305-e5d83a839148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: running, Stemmed: run, Lemmatized: running\n",
      "Original: wolves, Stemmed: wolv, Lemmatized: wolf\n",
      "Original: studies, Stemmed: studi, Lemmatized: study\n",
      "Original: children, Stemmed: children, Lemmatized: child\n",
      "Original: better, Stemmed: better, Lemmatized: better\n",
      "Original: flying, Stemmed: fli, Lemmatized: flying\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/mohamed/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/mohamed/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# List of words to compare\n",
    "words = [\"running\", \"wolves\", \"studies\", \"children\", \"better\", \"flying\"]\n",
    "# Compare stemming and lemmatization\n",
    "for word in words:\n",
    "    stemmed_word = stemmer.stem(word)\n",
    "    lemmatized_word = lemmatizer.lemmatize(word, pos='n')  # Noun as default POS\n",
    "    print(f\"Original: {word}, Stemmed: {stemmed_word}, Lemmatized: {lemmatized_word}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f05453a-e853-49ba-9dd9-d7d69fc49903",
   "metadata": {},
   "source": [
    "# POS Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3452e7b0-ccce-4e2e-b2b1-cc6f670e064f",
   "metadata": {},
   "source": [
    "## Exo 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5a1554-f648-4f2e-8777-b8cc509cca46",
   "metadata": {},
   "source": [
    "### Problem: Write a Python script that tags the parts of speech for each word in a simple sentence using spaCy. Display the word along with its POS tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2c0d737-ca04-4270-9838-ebba4874097b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The: DET\n",
      "quick: ADJ\n",
      "brown: ADJ\n",
      "fox: NOUN\n",
      "jumps: VERB\n",
      "over: ADP\n",
      "the: DET\n",
      "lazy: ADJ\n",
      "dog: NOUN\n",
      ".: PUNCT\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "doc = nlp(sentence)\n",
    "for token in doc:\n",
    "    print(f\"{token.text}: {token.pos_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915ab1d7-b5ee-4436-9d85-673e4b422fa3",
   "metadata": {},
   "source": [
    "## Exo 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b6d3df-9efc-4f8f-9f03-291d868e45fa",
   "metadata": {},
   "source": [
    "### Problem: Write a Python script that tags the parts of speech for each word in a paragraph using spaCy. Then, count how many nouns, verbs, adjectives, and adverbs are present in the paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e93cce05-f42c-437d-97d7-298b6df31946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nouns: 3, Verbs: 1, Adjectives: 5, Adverbs: 1\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "paragraph = \"The quick brown fox jumps over the lazy dog. The fox is very quick and agile.\"\n",
    "doc = nlp(paragraph)\n",
    "pos_counts = Counter(token.pos_ for token in doc)\n",
    "nouns = pos_counts['NOUN']\n",
    "verbs = pos_counts['VERB']\n",
    "adjectives = pos_counts['ADJ']\n",
    "adverbs = pos_counts['ADV']\n",
    "print(f\"Nouns: {nouns}, Verbs: {verbs}, Adjectives: {adjectives}, Adverbs: {adverbs}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926aceb5-3123-4a28-a3b6-a5feb57c8011",
   "metadata": {},
   "source": [
    "## Exo 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dc72f0-81aa-4cac-ba96-46f0a833ebb8",
   "metadata": {},
   "source": [
    "### Problem: Write a Python script that tags the parts of speech for each word in a sentence using spaCy. Then, extract and display all the nouns and verbs separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f6fff90-d532-4150-9bc6-d1363b32b5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"The programmer quickly wrote code and fixed bugs in the software.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a30b06-7b61-4f82-adfa-f1f2f9e1405b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
